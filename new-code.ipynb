{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "235e7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle.nn import Transformer\n",
    "from paddle.io import Dataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79bbca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_path='camel_code.txt'\n",
    "comment_path='comment.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bb0df78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_dataset(a,b):\n",
    "    # a : code\n",
    "    # b: comment\n",
    "    with open(a,encoding='utf-8') as tc:\n",
    "        lines1=tc.readlines()\n",
    "        for i in range(len(lines1)):\n",
    "             lines1[i]=\"<start> \"+lines1[i].strip('\\n')+\" <end>\"\n",
    "    with open(b,encoding='utf-8') as ts:\n",
    "        lines2=ts.readlines()\n",
    "        for i in range(len(lines2)):\n",
    "            lines2[i]=\"<start> \"+lines2[i].strip('\\n')+\" <end>\"\n",
    "    if(len(lines1)!=len(lines2) ):\n",
    "        print(\"Data volume does not match\")\n",
    "    return lines1,lines2\n",
    "\n",
    "code,comment=creat_dataset(code_path,comment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b1599ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> public synchronized void info ( string msg ) { log record record = new log record ( level . info , msg ) ; log ( record ) ; } <end>\n",
      "<start> logs a info message <end>\n"
     ]
    }
   ],
   "source": [
    "print(code[0])\n",
    "print(comment[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d15def1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cropus(data):\n",
    "    crpous=[]\n",
    "    for i in range(len(data)):\n",
    "        cr=data[i].strip().lower()\n",
    "        cr=cr.split()\n",
    "        crpous.extend(cr)\n",
    "    return crpous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca65ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造词典，统计每个词的频率，并根据频率将每个词转换为一个整数id\n",
    "def build_dict(corpus,frequency):\n",
    "    # 首先统计每个不同词的频率（出现的次数），使用一个词典记录\n",
    "    word_freq_dict = dict()\n",
    "    for word in corpus:\n",
    "        if word not in word_freq_dict:\n",
    "            word_freq_dict[word] = 0\n",
    "        word_freq_dict[word] += 1\n",
    "\n",
    "    # 将这个词典中的词，按照出现次数排序，出现次数越高，排序越靠前\n",
    "    # 一般来说，出现频率高的高频词往往是：I，the，you这种代词，而出现频率低的词，往往是一些名词，如：nlp\n",
    "    word_freq_dict = sorted(word_freq_dict.items(), key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "    # 构造3个不同的词典，分别存储，\n",
    "    # 每个词到id的映射关系：word2id_dict\n",
    "    \n",
    "    # 每个id到词的映射关系：id2word_dict\n",
    "    word2id_dict = {'<pad>':0,'<unk>':1}\n",
    "   \n",
    "    id2word_dict = {0:'<pad>',1:'<unk>'}\n",
    "\n",
    "    # 按照频率，从高到低，开始遍历每个单词，并为这个单词构造一个独一无二的id\n",
    "    for word, freq in word_freq_dict:\n",
    "        if freq>frequency:\n",
    "            curr_id = len(word2id_dict)\n",
    "            word2id_dict[word] = curr_id\n",
    "            id2word_dict[curr_id] = word\n",
    "        else:\n",
    "            word2id_dict[word]=1\n",
    "    return word2id_dict, id2word_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "239be07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_fre=2\n",
    "code_word2id_dict,code_id2word_dict=build_dict(build_cropus(code),word_fre)\n",
    "comment_word2id_dict,comment_id2word_dict=build_dict(build_cropus(comment),word_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e1086f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31921\n",
      "22256\n"
     ]
    }
   ],
   "source": [
    "code_maxlen=200\n",
    "comment_maxlen=30\n",
    "code_vocab_size=len(code_id2word_dict)\n",
    "comment_vocab_size=len(comment_id2word_dict)\n",
    "print(code_vocab_size)\n",
    "print(comment_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8384e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tensor(data,dicta,maxlen):\n",
    "    tensor=[]\n",
    "    for i in range(len(data)):\n",
    "        subtensor=[]\n",
    "        lista=data[i].split()\n",
    "        for j in range(len(lista)):\n",
    "            index=dicta.get(lista[j])\n",
    "            subtensor.append(index)\n",
    "    \n",
    "        if len(subtensor) < maxlen:\n",
    "            subtensor+=[0]*(maxlen-len(subtensor))\n",
    "        else:\n",
    "            subtensor=subtensor[:maxlen]\n",
    "\n",
    "        tensor.append(subtensor)\n",
    "    return tensor\n",
    "\n",
    "code_tensor=build_tensor(code,code_word2id_dict,code_maxlen)\n",
    "comment_tensor=build_tensor(comment,comment_word2id_dict,comment_maxlen)\n",
    "code_tensor=np.array(code_tensor)\n",
    "comment_tensor=np.array(comment_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29baacdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "(20000, 200)\n",
      "(445812, 200)\n"
     ]
    }
   ],
   "source": [
    "test_code_tensor=code_tensor[:20000]\n",
    "val_code_tensor=code_tensor[20000:40000]\n",
    "train_code_tensor=code_tensor[40000:]\n",
    "\n",
    "test_comment_tensor=comment_tensor[:20000]\n",
    "val_comment_tensor=comment_tensor[20000:40000]\n",
    "train_comment_tensor=comment_tensor[40000:]\n",
    "\n",
    "print(test_code_tensor.shape[0])\n",
    "print(val_code_tensor.shape)\n",
    "print(train_code_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f76be8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3482\n",
      "156\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    步骤一：继承paddle.io.Dataset类\n",
    "    \"\"\"\n",
    "    def __init__(self, code,comment):\n",
    "        \"\"\"\n",
    "        步骤二：实现构造函数，定义数据集大小\n",
    "        \"\"\"\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.code = code\n",
    "        self.comment=comment\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        步骤三：实现__getitem__方法，定义指定index时如何获取数据，并返回单条数据（训练数据，对应的标签）\n",
    "        \"\"\"\n",
    "        return self.code[index], self.comment[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        步骤四：实现__len__方法，返回数据集总数目\n",
    "        \"\"\"\n",
    "        return self.code.shape[0]\n",
    "\n",
    "BATCH_SIZE=128\n",
    "\n",
    "train_batch_num=train_code_tensor.shape[0]//BATCH_SIZE #3482\n",
    "val_batch_num=val_code_tensor.shape[0]//BATCH_SIZE #156\n",
    "print(train_batch_num)\n",
    "print(val_batch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c853ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试定义的数据集\n",
    "train_dataset = MyDataset(train_code_tensor,train_comment_tensor)\n",
    "train_loader = paddle.io.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)\n",
    "val_dataset=MyDataset(val_code_tensor,val_comment_tensor)\n",
    "val_loader=paddle.io.DataLoader(val_dataset,batch_size=BATCH_SIZE,shuffle=True,drop_last=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e52db2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos,i,d_model):\n",
    "    angle_rate=1/np.power(10000,(2*(i//2))/np.float32(d_model))\n",
    "    return pos*angle_rate\n",
    "\n",
    "def get_position_embedding(sentence_length,d_model):\n",
    "    angle_rads=get_angles(np.arange(sentence_length)[:,np.newaxis],\n",
    "                         np.arange(d_model)[np.newaxis,:],\n",
    "                         d_model)\n",
    "    sines=np.sin(angle_rads[:,0::2])\n",
    "    cosines=np.cos(angle_rads[:,1::2])\n",
    "    \n",
    "    position_embedding=np.concatenate([sines,cosines],axis=-1)\n",
    "    position_embedding=paddle.to_tensor(position_embedding[np.newaxis,...])\n",
    "    \n",
    "    return paddle.cast(position_embedding,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "085604bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    zeo=paddle.zeros(seq.shape,seq.dtype)\n",
    "    padding_mask=paddle.cast(paddle.equal(seq,zeo),dtype='float32')\n",
    "    return paddle.unsqueeze(padding_mask,axis=[1,2])\n",
    "\n",
    "def create_look_ahead_mask(length):\n",
    "    return paddle.tensor.triu((paddle.ones((length, length))),1)\n",
    "\n",
    "def creat_mask(inp,tar):\n",
    "    encoder_padding_mask=create_padding_mask(inp)\n",
    "    encoder_decoder_padding_mask=create_padding_mask(inp)\n",
    "    \n",
    "    look_ahead_mask=create_look_ahead_mask(tar.shape[1])\n",
    "    decoder_padding_mask=create_padding_mask(tar)\n",
    "    deocder_mask=paddle.maximum(decoder_padding_mask,look_ahead_mask)\n",
    "    \n",
    "    return encoder_padding_mask,deocder_mask,encoder_decoder_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4e8713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    " \n",
    "    # 相乘前转置y\n",
    "    matmul_qk = paddle.matmul(q, k, transpose_y=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # 缩放 matmul_qk\n",
    "    dk = paddle.cast(paddle.shape(k)[-1], dtype='float32')\n",
    "    scaled_attention_logits = matmul_qk / paddle.sqrt(dk)\n",
    "\n",
    "    # 将 mask 加入到缩放的张量上。\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax 在最后一个轴（seq_len_k）上归一化，因此分数\n",
    "    # 相加等于1。\n",
    "    attention_weights = paddle.nn.functional.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = paddle.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f91764b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 60, 512]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(paddle.nn.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = paddle.nn.Linear(d_model,d_model)\n",
    "        self.wk = paddle.nn.Linear(d_model,d_model)\n",
    "        self.wv = paddle.nn.Linear(d_model,d_model)\n",
    "\n",
    "        self.dense = paddle.nn.Linear(d_model,d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"分拆最后一个维度到 (num_heads, depth).\n",
    "        转置结果使得形状为 (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = paddle.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return paddle.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def forward(self, v, k, q, mask):\n",
    "        \n",
    "        batch_size = q.shape[0]\n",
    "       \n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention = scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        scaled_attention = paddle.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = paddle.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "                                    \n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output\n",
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "#y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "y= paddle.uniform((1,60,512))\n",
    "out= temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16778d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return paddle.nn.Sequential(\n",
    "        paddle.nn.Linear(d_model,dff),  # (batch_size, seq_len, dff)\n",
    "        paddle.nn.ReLU(),\n",
    "        paddle.nn.Linear(dff,d_model)  # (batch_size, seq_len, d_model)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a579f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(paddle.nn.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        # 如果是单个整数，则此模块将在最后一个维度上规范化（此时最后一维的维度需与该参数相同）\n",
    "        self.layernorm1 = paddle.nn.LayerNorm(d_model,epsilon=1e-6)\n",
    "        self.layernorm2 = paddle.nn.LayerNorm(d_model,epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = paddle.nn.Dropout(p=rate)\n",
    "        self.dropout2 = paddle.nn.Dropout(p=rate)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        attn_output= self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "180011ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(paddle.nn.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = paddle.nn.LayerNorm(d_model,epsilon=1e-6)\n",
    "        self.layernorm2 = paddle.nn.LayerNorm(d_model,epsilon=1e-6)\n",
    "        self.layernorm3 = paddle.nn.LayerNorm(d_model,epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = paddle.nn.Dropout(p=rate)\n",
    "        self.dropout2 = paddle.nn.Dropout(p=rate)\n",
    "        self.dropout3 = paddle.nn.Dropout(p=rate)\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "            \n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1= self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2= self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "            \n",
    "        attn2 = self.dropout2(attn2)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0816dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(paddle.nn.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,maximum_position_encoding, rate=0.1):\n",
    "                \n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = paddle.nn.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = get_position_embedding(maximum_position_encoding,self.d_model) \n",
    "                                                \n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "                        \n",
    "\n",
    "        self.dropout =paddle.nn.Dropout(p=rate)\n",
    "\n",
    "    def forward(self, x,  mask):\n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "\n",
    "        # 将嵌入和位置编码相加。\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "     \n",
    "        x *= np.sqrt(self.d_model).astype(np.float32)\n",
    "        \n",
    "       \n",
    "        x +=self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f18a0af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(paddle.nn.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = paddle.nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = get_position_embedding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "                        \n",
    "        self.dropout = paddle.nn.Dropout(p=rate)\n",
    "\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "            \n",
    "        seq_len = x.shape[1]\n",
    "       \n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= np.sqrt(self.d_model).astype(np.float32)\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x= self.dec_layers[i](x, enc_output,look_ahead_mask, padding_mask)\n",
    "                                                \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x\n",
    "class Trans(paddle.nn.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,target_vocab_size, pe_input, pe_target, rate=0.1): \n",
    "                \n",
    "        super(Trans, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "                            \n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "                            \n",
    "        self.final_layer = paddle.nn.Linear(d_model,target_vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, inp, tar, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "            \n",
    "        enc_output = self.encoder(inp,  enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output = self.decoder(tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
    "            \n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5eb3fa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 1, 1, 200]\n",
      "[128, 1, 29, 29]\n",
      "[128, 1, 1, 200]\n"
     ]
    }
   ],
   "source": [
    "num_layers = 4\n",
    "d_model = 256\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "dropout_rate = 0.2\n",
    "\n",
    "trans = Trans(num_layers,d_model,num_heads,dff, \n",
    "code_vocab_size,comment_vocab_size,pe_input=code_vocab_size, pe_target=comment_vocab_size) \n",
    "\n",
    "for batch_id, data in enumerate(train_loader()):\n",
    "    x_data = data[0]\n",
    "    y_data = data[1]\n",
    "    y_inp=y_data[:,:-1]\n",
    "    y_real=y_data[:,1:]#[batch_size,seq_len]\n",
    "    encoder_padding_mask,deocder_mask,encoder_decoder_padding_mask=creat_mask(x_data,y_inp)\n",
    "    print(encoder_padding_mask.shape)\n",
    "    print(deocder_mask.shape)\n",
    "    print(encoder_decoder_padding_mask.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ba9cf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------------------------\n",
      " Layer (type)                             Input Shape                                Output Shape         Param #    \n",
      "=======================================================================================================================\n",
      "  Embedding-1                             [[128, 200]]                             [128, 200, 256]       8,171,776   \n",
      "   Dropout-9                           [[128, 200, 256]]                           [128, 200, 256]           0       \n",
      "   Encoder-1                     [[128, 200], [128, 1, 1, 200]]                    [128, 200, 256]           0       \n",
      "  Embedding-2                             [[128, 29]]                               [128, 29, 256]       5,697,536   \n",
      "  Dropout-22                            [[128, 29, 256]]                            [128, 29, 256]           0       \n",
      "   Decoder-1    [[128, 29], [128, 200, 256], [128, 1, 29, 29], [128, 1, 1, 200]]    [128, 29, 256]           0       \n",
      "   Linear-69                            [[128, 29, 256]]                           [128, 29, 22256]      5,719,792   \n",
      "=======================================================================================================================\n",
      "Total params: 19,589,104\n",
      "Trainable params: 19,589,104\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.72\n",
      "Forward/backward pass size (MB): 802.05\n",
      "Params size (MB): 74.73\n",
      "Estimated Total Size (MB): 877.49\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "batch= 0\n",
      "batch= 100\n",
      "batch= 200\n",
      "batch= 300\n",
      "batch= 400\n",
      "batch= 500\n",
      "batch= 600\n",
      "batch= 700\n",
      "batch= 800\n",
      "batch= 900\n",
      "batch= 1000\n",
      "batch= 1100\n",
      "batch= 1200\n",
      "batch= 1300\n",
      "batch= 1400\n",
      "batch= 1500\n",
      "batch= 1600\n",
      "batch= 1700\n",
      "batch= 1800\n",
      "batch= 1900\n",
      "batch= 2000\n",
      "batch= 2100\n",
      "batch= 2200\n",
      "batch= 2300\n",
      "batch= 2400\n",
      "batch= 2500\n",
      "batch= 2600\n",
      "batch= 2700\n",
      "batch= 2800\n",
      "batch= 2900\n",
      "batch= 3000\n",
      "batch= 3100\n",
      "batch= 3200\n",
      "batch= 3300\n",
      "batch= 3400\n"
     ]
    }
   ],
   "source": [
    "paddle.summary(trans,[(128,200),(128,29),(128, 1, 1, 200),(128, 1, 29, 29),(128, 1, 1, 200)],dtypes='int64')\n",
    "for batch_id, data in enumerate(train_loader()):\n",
    "     if batch_id%100==0:\n",
    "                print('batch=',batch_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68fd1412",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=2\n",
    "\n",
    "def draw_loss(a,b):\n",
    "    x_list=[]\n",
    "    for i in range(len(a)):\n",
    "        x_list.append(i)\n",
    "    plt.title(\"LOSS\")\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.plot(x_list,a,marker='s',label=\"train\")\n",
    "    plt.plot(x_list,b,marker='s',label=\"val\")\n",
    "    plt.legend()\n",
    "    plt.savefig('LOSS.png')\n",
    "    plt.show()\n",
    "\n",
    "def train():\n",
    "    #clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=5.0)\n",
    "    scheduler = paddle.optimizer.lr.NoamDecay(d_model, warmup_steps=4000)# ,verbose=True\n",
    "    # opt=paddle.optimizer.Adam(learning_rate=scheduler,beta1=0.9, beta2=0.98, epsilon=1e-09,parameters=trans.parameters(),grad_clip=clip)\n",
    "    \n",
    "    opt=paddle.optimizer.Adam(learning_rate=scheduler,beta1=0.9, beta2=0.98, epsilon=1e-09,parameters=trans.parameters())\n",
    "\n",
    "    ce_loss = paddle.nn.CrossEntropyLoss(reduction='none')\n",
    "    train_loss=[]\n",
    "    val_loss=[]\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch:{}\".format(epoch))\n",
    "        train_epoch_loss=0\n",
    "\n",
    "        # 此处声明模型训练，使用drpout\n",
    "        trans.train()\n",
    "\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            x_data = data[0]\n",
    "            y_data = data[1]\n",
    "            y_inp=y_data[:,:-1]\n",
    "            y_real=y_data[:,1:]#[batch_size,seq_len]\n",
    "            \n",
    "            \n",
    "             encoder_padding_mask,deocder_mask,encoder_decoder_padding_mask=creat_mask(x_data,y_inp)\n",
    "            \n",
    "            pre=trans(x_data,y_inp,encoder_padding_mask,deocder_mask,encoder_decoder_padding_mask)\n",
    "            #[batch_size,seq_len,vocab_size]\n",
    "            #print(y_real.dtype)\n",
    "            y_real = y_real.astype('int64')\n",
    "            batch_loss=ce_loss(pre,y_real)\n",
    "            # 消除padding的0的影响\n",
    "            zeo=paddle.zeros(y_real.shape,y_real.dtype)\n",
    "            mask=paddle.cast(paddle.logical_not(paddle.equal(y_real,zeo)),dtype=pre.dtype)\n",
    "            batch_loss*=mask\n",
    "            batch_loss=paddle.mean(batch_loss)\n",
    "\n",
    "            train_epoch_loss+=batch_loss.numpy()\n",
    "\n",
    "            batch_loss.backward()\n",
    "\n",
    "            \n",
    "            #print('batch=',batch_id,' batch_loss= ',batch_loss.numpy())\n",
    "            # 更新参数\n",
    "            opt.step()\n",
    "\n",
    "            # 梯度清零\n",
    "            opt.clear_grad()\n",
    "            #学习率更新\n",
    "            scheduler.step()\n",
    "            #break\n",
    "        ava_loss=train_epoch_loss/train_batch_num\n",
    "        train_loss.append(ava_loss)\n",
    "        \n",
    "\n",
    "        print(\"train epoch: {}  AVALOSS: {}\\n\".format(epoch,ava_loss))\n",
    "        #break\n",
    "\n",
    "        val_epoch_loss=0\n",
    "        # 声明模型在预测，不使用dropout\n",
    "        trans.eval()\n",
    "        for batch_id, data in enumerate(val_loader()):\n",
    "            x_data = data[0]\n",
    "            y_data = data[1]\n",
    "            y_inp=y_data[:,:-1]\n",
    "            y_real=y_data[:,1:]\n",
    "           \n",
    "            encoder_padding_mask,deocder_mask,encoder_decoder_padding_mask=creat_mask(x_data,y_inp)\n",
    "\n",
    "            pre=trans(x_data,y_inp,encoder_padding_mask,deocder_mask,encoder_decoder_padding_mask)\n",
    "            y_real = y_real.astype('int64')\n",
    "            batch_loss=ce_loss(pre,y_real)\n",
    "            # 消除padding 的0的影响\n",
    "            zeo=paddle.zeros(y_real.shape,y_real.dtype)\n",
    "            mask=paddle.cast(paddle.logical_not(paddle.equal(y_real,zeo)),dtype=pre.dtype)\n",
    "            batch_loss*=mask\n",
    "            batch_loss=paddle.mean(batch_loss)\n",
    "\n",
    "            val_epoch_loss+=batch_loss.numpy()\n",
    "\n",
    "            \n",
    "            #print('batch=',batch_id,' batch_loss= ',batch_loss.numpy())\n",
    "        ava_loss=val_epoch_loss/val_batch_num\n",
    "        \n",
    "        val_loss.append(ava_loss)\n",
    "        print(\"val epoch: {}  AVALOSS: {}\\n\".format(epoch,ava_loss))\n",
    "        \n",
    "    # 至此，训练结束。下面绘制loss图\n",
    "    draw_loss(train_loss,val_loss)\n",
    "\n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca303e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gao taiyu\\AppData\\Roaming\\Python\\Python39\\site-packages\\paddle\\fluid\\dygraph\\math_op_patch.py:73: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  out = core.ops.fill_constant(out, 'dtype', dtype, 'shape', shape,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch= 0  batch_loss=  [3.8124182]\n",
      "batch= 100  batch_loss=  [4.158208]\n",
      "batch= 200  batch_loss=  [3.7337093]\n",
      "batch= 300  batch_loss=  [3.8947823]\n",
      "batch= 400  batch_loss=  [3.727067]\n",
      "batch= 500  batch_loss=  [3.731418]\n",
      "batch= 600  batch_loss=  [4.1078815]\n",
      "batch= 700  batch_loss=  [3.7346907]\n",
      "batch= 800  batch_loss=  [3.9407213]\n",
      "batch= 900  batch_loss=  [3.8382058]\n",
      "batch= 1000  batch_loss=  [3.9398248]\n",
      "batch= 1100  batch_loss=  [4.123186]\n",
      "batch= 1200  batch_loss=  [3.8168561]\n",
      "batch= 1300  batch_loss=  [3.8385425]\n",
      "batch= 1400  batch_loss=  [3.8334982]\n",
      "batch= 1500  batch_loss=  [3.9503074]\n",
      "batch= 1600  batch_loss=  [3.6005743]\n",
      "batch= 1700  batch_loss=  [4.149143]\n",
      "batch= 1800  batch_loss=  [3.97402]\n",
      "batch= 1900  batch_loss=  [3.7685347]\n",
      "batch= 2000  batch_loss=  [3.9916303]\n",
      "batch= 2100  batch_loss=  [3.921741]\n",
      "batch= 2200  batch_loss=  [3.849634]\n",
      "batch= 2300  batch_loss=  [3.9011202]\n",
      "batch= 2400  batch_loss=  [3.8328252]\n",
      "batch= 2500  batch_loss=  [4.2174406]\n",
      "batch= 2600  batch_loss=  [3.8797362]\n",
      "batch= 2700  batch_loss=  [3.9943511]\n",
      "batch= 2800  batch_loss=  [3.869689]\n",
      "batch= 2900  batch_loss=  [3.6267617]\n",
      "batch= 3000  batch_loss=  [4.1030912]\n",
      "batch= 3100  batch_loss=  [3.970973]\n",
      "batch= 3200  batch_loss=  [3.9926372]\n",
      "batch= 3300  batch_loss=  [3.8030126]\n",
      "batch= 3400  batch_loss=  [3.9318273]\n",
      "train epoch: 0  AVALOSS: [0.03921885]\n",
      "\n",
      "batch= 0  batch_loss=  [3.959047]\n",
      "batch= 100  batch_loss=  [3.9603276]\n",
      "val epoch: 0  AVALOSS: [0.05076522]\n",
      "\n",
      "epoch:1\n",
      "batch= 0  batch_loss=  [3.6719635]\n",
      "batch= 100  batch_loss=  [3.7760262]\n",
      "batch= 200  batch_loss=  [3.8319547]\n",
      "batch= 300  batch_loss=  [3.7210915]\n",
      "batch= 400  batch_loss=  [4.0213175]\n",
      "batch= 500  batch_loss=  [4.186158]\n",
      "batch= 600  batch_loss=  [3.9814417]\n",
      "batch= 700  batch_loss=  [3.9124875]\n",
      "batch= 800  batch_loss=  [3.93798]\n",
      "batch= 900  batch_loss=  [3.8592842]\n",
      "batch= 1000  batch_loss=  [4.282294]\n",
      "batch= 1100  batch_loss=  [3.761928]\n",
      "batch= 1200  batch_loss=  [4.0356107]\n",
      "batch= 1300  batch_loss=  [4.072595]\n",
      "batch= 1400  batch_loss=  [4.074381]\n",
      "batch= 1500  batch_loss=  [3.8356194]\n",
      "batch= 1600  batch_loss=  [3.9408748]\n",
      "batch= 1700  batch_loss=  [3.9342034]\n",
      "batch= 1800  batch_loss=  [4.2256093]\n",
      "batch= 1900  batch_loss=  [3.9520242]\n",
      "batch= 2000  batch_loss=  [3.8949056]\n",
      "batch= 2100  batch_loss=  [3.49065]\n",
      "batch= 2200  batch_loss=  [3.845569]\n",
      "batch= 2300  batch_loss=  [3.8394659]\n",
      "batch= 2400  batch_loss=  [3.8187933]\n",
      "batch= 2500  batch_loss=  [3.8262558]\n",
      "batch= 2600  batch_loss=  [3.7588375]\n",
      "batch= 2700  batch_loss=  [3.972359]\n",
      "batch= 2800  batch_loss=  [4.0418777]\n",
      "batch= 2900  batch_loss=  [3.9221487]\n",
      "batch= 3000  batch_loss=  [4.0383654]\n",
      "batch= 3100  batch_loss=  [3.9437919]\n",
      "batch= 3200  batch_loss=  [3.7880716]\n",
      "batch= 3300  batch_loss=  [3.8207693]\n",
      "batch= 3400  batch_loss=  [3.716742]\n",
      "train epoch: 1  AVALOSS: [0.03926865]\n",
      "\n",
      "batch= 0  batch_loss=  [3.9978216]\n",
      "batch= 100  batch_loss=  [3.9830034]\n",
      "val epoch: 1  AVALOSS: [0.05115914]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a2a34f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paddle.save(trans.state_dict(), \"trans_net.pdparams\")\n",
    "#paddle.save(opt.state_dict(), \"opt.pdopt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7960bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 20000/20000 [11:11:41<00:00,  2.02s/it]\n"
     ]
    }
   ],
   "source": [
    " def evalute(code):\n",
    "        result=''\n",
    "        # code.shape(1,500)\n",
    "        code=paddle.unsqueeze(code,axis=0)\n",
    "        \n",
    "        # decoder_input.shape(1,1)\n",
    "        decoder_input=paddle.unsqueeze(paddle.to_tensor([comment_word2id_dict['<start>']]),axis=0)\n",
    "        \n",
    "        for i in range(comment_maxlen):\n",
    "            encoder_padding_mask,decoder_mask,encoder_decoder_padding_mask=creat_mask(code,decoder_input)\n",
    "            #(batch_size,output_target_len,target_vocab_size)\n",
    "            pre=trans(code,decoder_input,encoder_padding_mask,decoder_mask,encoder_decoder_padding_mask)\n",
    "            \n",
    "            pre=pre[:,-1:,:]\n",
    "            \n",
    "            pre_id=paddle.argmax(pre,axis=-1)\n",
    "           # print(pre_id)\n",
    "            predicted_id = paddle.cast(pre_id, dtype='int64')\n",
    "          #  print(predicted_id)\n",
    "            pre_id=pre_id.numpy()[0][0]\n",
    "           # print(pre_id)\n",
    "            if comment_id2word_dict[pre_id]=='<end>':\n",
    "                return result\n",
    "            \n",
    "            result+=comment_id2word_dict[pre_id]+' '\n",
    "            decoder_input = decoder_input.astype('int64')\n",
    "            decoder_input=paddle.concat(x=[decoder_input,predicted_id],axis=-1)\n",
    "           \n",
    "        return result\n",
    "\n",
    "def translate():\n",
    "    with open('result.txt','w+') as re:\n",
    "        for i in tqdm(range(len(test_code_tensor))):\n",
    "        #for i in range(100):    \n",
    "            result=evalute(paddle.to_tensor(test_code_tensor[i]))\n",
    "            re.write(result+'\\n')\n",
    "            #print(result)\n",
    "translate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "835a07b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x90 in position 1943: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     pre\u001b[38;5;241m=\u001b[39mre\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(code_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m scode:\n\u001b[1;32m----> 5\u001b[0m     code\u001b[38;5;241m=\u001b[39m\u001b[43mscode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(comment_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m scomment:\n\u001b[0;32m      8\u001b[0m     comment\u001b[38;5;241m=\u001b[39mscomment\u001b[38;5;241m.\u001b[39mreadlines()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x90 in position 1943: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "with open('result.txt','r') as re:\n",
    "    pre=re.readlines()\n",
    "\n",
    "with open(code_path,'r') as scode:\n",
    "    code=scode.readlines()\n",
    "\n",
    "with open(comment_path,'r') as scomment:\n",
    "    comment=scomment.readlines()\n",
    "\n",
    "for i in range(5):\n",
    "    print('code: ',code[i].strip())\n",
    "    print('真实注释：',comment[i].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e9cd2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code:  <start> public synchronized void info ( string msg ) { log record record = new log record ( level . info , msg ) ; log ( record ) ; } <end>\n",
      "真实注释： logs a info message\n",
      "预测注释： viewpoint doc filter l possible doc doc world initialized filter wss doc doc doc que twitterlisttimeline ha deselect booleanliteral doc doc filter doc backwards viewpoint ha smartphone hubs doc doc \n",
      "\n",
      "code:  <start> public void handle gateway receiver create ( gateway receiver recv ) throws management exception { if ( ! is service initialised ( str_ ) ) { return ; } if ( ! recv . is manual start ( ) ) { return ; } create gateway receiver m bean ( recv ) ; } <end>\n",
      "真实注释： handles gateway receiver creation\n",
      "预测注释： initialized initialized under ormlite doc highlight httpexception highlight highlight highlight enforce que deselect ha ormlite ormlite injecting deselect starting injecting bytebuffer highlight doc under doc ormlite document deselect doc username \n",
      "\n",
      "code:  <start> public void data changed ( i data provider data provider ) ; <end>\n",
      "真实注释： this method will be notified by data provider whenever the data changed in data provider\n",
      "预测注释： starting supported world que initialized que filter initialized skipping filter world supported observer world world observer observer supported world world chronix doc twitterlisttimeline world world modifier initialized observer starting ends \n",
      "\n",
      "code:  <start> public void range ( i hypercube space , i visit kd node visitor ) { if ( root == null ) { return ; } root . range ( space , visitor ) ; } <end>\n",
      "真实注释： locate all points within the twodtree that fall within the given ihypercube and visit those nodes via the given visitor .\n",
      "预测注释： ormlite starting ha ormlite observer starting simple number que backwards represents possible deselect campo center equality doc ha possible highlight simple values under doc ormlite injecting values starting deselect filter \n",
      "\n",
      "code:  <start> public void handle disk creation ( disk store disk ) throws management exception { if ( ! is service initialised ( str_ ) ) { return ; } disk store m bean bridge bridge = new disk store m bean bridge ( disk ) ; disk store mx bean disk store m bean = new disk store m bean ( bridge ) ; object name disk store m bean name = m bean jmx adapter . get disk store m bean name ( cache impl . get distributed system ( ) . get distributed member ( ) , disk . get name ( ) ) ; object name changed m bean name = service . register internal m bean ( disk store m bean , disk store m bean name ) ; service . federate ( changed m bean name , disk store mx bean . class , bool_ ) ; notification notification = new notification ( jmx notification type . dis k_ stor e_ created , member source , sequence number . next ( ) , system . current time millis ( ) , management constants . dis k_ stor e_ create d_ prefix + disk . get name ( ) ) ; member level notif emitter . send notification ( notification ) ; member m bean bridge . add disk store ( disk ) ; } <end>\n",
      "真实注释： handles disk creation .\n",
      "预测注释： input input ontextchanged means ontextchanged que twitterlisttimeline jsonarrays speeds pinner item importing input might twitterlisttimeline record asleep filter que asleep might might asleep granted might might record record quasiliteral might \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('result.txt','r') as re:\n",
    "    pre=re.readlines()\n",
    "\n",
    "#with open(code_path,'r') as scode:\n",
    "#    code=scode.readlines()\n",
    "\n",
    "with open(comment_path,'r') as scomment:\n",
    "    comment=scomment.readlines()\n",
    "\n",
    "for i in range(5):\n",
    "    print('code: ',code[i].strip())\n",
    "    print('真实注释：',comment[i].strip())\n",
    "    print('预测注释：',pre[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab8250d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
