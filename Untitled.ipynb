{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2bcc8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle.nn import Transformer\n",
    "from paddle.io import Dataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1152c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_path='camel_code.txt'\n",
    "comment_path='comment.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d649cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_dataset(a,b):\n",
    "    # a : code\n",
    "    # b: comment\n",
    "    with open(a,encoding='utf-8') as tc:\n",
    "        lines1=tc.readlines()\n",
    "        for i in range(len(lines1)):\n",
    "             lines1[i]=\"<start> \"+lines1[i].strip('\\n')+\" <end>\"\n",
    "    with open(b,encoding='utf-8') as ts:\n",
    "        lines2=ts.readlines()\n",
    "        for i in range(len(lines2)):\n",
    "            lines2[i]=\"<start> \"+lines2[i].strip('\\n')+\" <end>\"\n",
    "    if(len(lines1)!=len(lines2) ):\n",
    "        print(\"数据量不匹配\")\n",
    "    return lines1,lines2\n",
    "\n",
    "code,comment=creat_dataset(code_path,comment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4286b6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> public synchronized void info ( string msg ) { log record record = new log record ( level . info , msg ) ; log ( record ) ; } <end>\n",
      "<start> logs a info message <end>\n"
     ]
    }
   ],
   "source": [
    "print(code[0])\n",
    "print(comment[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e2e794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cropus(data):\n",
    "    crpous=[]\n",
    "    for i in range(len(data)):\n",
    "        cr=data[i].strip().lower()\n",
    "        cr=cr.split()\n",
    "        crpous.extend(cr)\n",
    "    return crpous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c9ee93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造词典，统计每个词的频率，并根据频率将每个词转换为一个整数id\n",
    "def build_dict(corpus,frequency):\n",
    "    # 首先统计每个不同词的频率（出现的次数），使用一个词典记录\n",
    "    word_freq_dict = dict()\n",
    "    for word in corpus:\n",
    "        if word not in word_freq_dict:\n",
    "            word_freq_dict[word] = 0\n",
    "        word_freq_dict[word] += 1\n",
    "\n",
    "    # 将这个词典中的词，按照出现次数排序，出现次数越高，排序越靠前\n",
    "    # 一般来说，出现频率高的高频词往往是：I，the，you这种代词，而出现频率低的词，往往是一些名词，如：nlp\n",
    "    word_freq_dict = sorted(word_freq_dict.items(), key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "    # 构造3个不同的词典，分别存储，\n",
    "    # 每个词到id的映射关系：word2id_dict\n",
    "    \n",
    "    # 每个id到词的映射关系：id2word_dict\n",
    "    word2id_dict = {'<pad>':0,'<unk>':1}\n",
    "   \n",
    "    id2word_dict = {0:'<pad>',1:'<unk>'}\n",
    "\n",
    "    # 按照频率，从高到低，开始遍历每个单词，并为这个单词构造一个独一无二的id\n",
    "    for word, freq in word_freq_dict:\n",
    "        if freq>frequency:\n",
    "            curr_id = len(word2id_dict)\n",
    "            word2id_dict[word] = curr_id\n",
    "            id2word_dict[curr_id] = word\n",
    "        else:\n",
    "            word2id_dict[word]=1\n",
    "    return word2id_dict, id2word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c596b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_fre=2\n",
    "code_word2id_dict,code_id2word_dict=build_dict(build_cropus(code),word_fre)\n",
    "comment_word2id_dict,comment_id2word_dict=build_dict(build_cropus(comment),word_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9df06ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31921\n",
      "22256\n"
     ]
    }
   ],
   "source": [
    "code_maxlen=200\n",
    "comment_maxlen=30\n",
    "code_vocab_size=len(code_id2word_dict)\n",
    "comment_vocab_size=len(comment_id2word_dict)\n",
    "print(code_vocab_size)\n",
    "print(comment_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15ef814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tensor(data,dicta,maxlen):\n",
    "    tensor=[]\n",
    "    for i in range(len(data)):\n",
    "        subtensor=[]\n",
    "        lista=data[i].split()\n",
    "        for j in range(len(lista)):\n",
    "            index=dicta.get(lista[j])\n",
    "            subtensor.append(index)\n",
    "    \n",
    "        if len(subtensor) < maxlen:\n",
    "            subtensor+=[0]*(maxlen-len(subtensor))\n",
    "        else:\n",
    "            subtensor=subtensor[:maxlen]\n",
    "\n",
    "        tensor.append(subtensor)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1849f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_tensor=build_tensor(code,code_word2id_dict,code_maxlen)\n",
    "comment_tensor=build_tensor(comment,comment_word2id_dict,comment_maxlen)\n",
    "code_tensor=np.array(code_tensor)\n",
    "comment_tensor=np.array(comment_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b07a6f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "(20000, 200)\n",
      "(445812, 200)\n"
     ]
    }
   ],
   "source": [
    "test_code_tensor=code_tensor[:20000]\n",
    "val_code_tensor=code_tensor[20000:40000]\n",
    "train_code_tensor=code_tensor[40000:]\n",
    "\n",
    "test_comment_tensor=comment_tensor[:20000]\n",
    "val_comment_tensor=comment_tensor[20000:40000]\n",
    "train_comment_tensor=comment_tensor[40000:]\n",
    "\n",
    "print(test_code_tensor.shape[0])\n",
    "print(val_code_tensor.shape)\n",
    "print(train_code_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ce67f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    步骤一：继承paddle.io.Dataset类\n",
    "    \"\"\"\n",
    "    def __init__(self, code,comment):\n",
    "        \"\"\"\n",
    "        步骤二：实现构造函数，定义数据集大小\n",
    "        \"\"\"\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.code = code\n",
    "        self.comment=comment\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        步骤三：实现__getitem__方法，定义指定index时如何获取数据，并返回单条数据（训练数据，对应的标签）\n",
    "        \"\"\"\n",
    "        return self.code[index], self.comment[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        步骤四：实现__len__方法，返回数据集总数目\n",
    "        \"\"\"\n",
    "        return self.code.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfd1a7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3482\n",
      "156\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE=128\n",
    "\n",
    "train_batch_num=train_code_tensor.shape[0]//BATCH_SIZE #3482\n",
    "val_batch_num=val_code_tensor.shape[0]//BATCH_SIZE #156\n",
    "print(train_batch_num)\n",
    "print(val_batch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd1c5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_code_tensor,train_comment_tensor)\n",
    "train_loader = paddle.io.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)\n",
    "val_dataset=MyDataset(val_code_tensor,val_comment_tensor)\n",
    "val_loader=paddle.io.DataLoader(val_dataset,batch_size=BATCH_SIZE,shuffle=True,drop_last=True)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87d73c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos,i,d_model):\n",
    "    angle_rate=1/np.power(10000,(2*(i//2))/np.float32(d_model))\n",
    "    return pos*angle_rate\n",
    "\n",
    "def get_position_embedding(sentence_length,d_model):\n",
    "    angle_rads=get_angles(np.arange(sentence_length)[:,np.newaxis],\n",
    "                         np.arange(d_model)[np.newaxis,:],\n",
    "                         d_model)\n",
    "    sines=np.sin(angle_rads[:,0::2])\n",
    "    cosines=np.cos(angle_rads[:,1::2])\n",
    "    \n",
    "    position_embedding=np.concatenate([sines,cosines],axis=-1)\n",
    "    position_embedding=paddle.to_tensor(position_embedding[np.newaxis,...])\n",
    "    \n",
    "    return paddle.cast(position_embedding,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e667b275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    zeo=paddle.zeros(seq.shape,seq.dtype)\n",
    "    padding_mask=paddle.cast(paddle.equal(seq,zeo),dtype='float32')\n",
    "    return paddle.unsqueeze(padding_mask,axis=[1,2])\n",
    "\n",
    "def create_look_ahead_mask(length):\n",
    "    return paddle.tensor.triu((paddle.ones((length, length))),1)\n",
    "\n",
    "def creat_mask(inp,tar):\n",
    "    encoder_padding_mask=create_padding_mask(inp)\n",
    "    encoder_decoder_padding_mask=create_padding_mask(inp)\n",
    "    \n",
    "    look_ahead_mask=create_look_ahead_mask(tar.shape[1])\n",
    "    decoder_padding_mask=create_padding_mask(tar)\n",
    "    deocder_mask=paddle.maximum(decoder_padding_mask,look_ahead_mask)\n",
    "    \n",
    "    return encoder_padding_mask,deocder_mask,encoder_decoder_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e0bd90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    " \n",
    "    # 相乘前转置y\n",
    "    matmul_qk = paddle.matmul(q, k, transpose_y=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # 缩放 matmul_qk\n",
    "    dk = paddle.cast(paddle.shape(k)[-1], dtype='float32')\n",
    "    scaled_attention_logits = matmul_qk / paddle.sqrt(dk)\n",
    "\n",
    "    # 将 mask 加入到缩放的张量上。\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax 在最后一个轴（seq_len_k）上归一化，因此分数\n",
    "    # 相加等于1。\n",
    "    attention_weights = paddle.nn.functional.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = paddle.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64238e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(paddle.nn.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = paddle.nn.Linear(d_model,d_model)\n",
    "        self.wk = paddle.nn.Linear(d_model,d_model)\n",
    "        self.wv = paddle.nn.Linear(d_model,d_model)\n",
    "\n",
    "        self.dense = paddle.nn.Linear(d_model,d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"分拆最后一个维度到 (num_heads, depth).\n",
    "        转置结果使得形状为 (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = paddle.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return paddle.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def forward(self, v, k, q, mask):\n",
    "        \n",
    "        batch_size = q.shape[0]\n",
    "       \n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention = scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        scaled_attention = paddle.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = paddle.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "                                    \n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43fb2a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 60, 512]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "#y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "y= paddle.uniform((1,60,512))\n",
    "out= temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f7db69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return paddle.nn.Sequential(\n",
    "        paddle.nn.Linear(d_model,dff),  # (batch_size, seq_len, dff)\n",
    "        paddle.nn.ReLU(),\n",
    "        paddle.nn.Linear(dff,d_model)  # (batch_size, seq_len, d_model)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bef73e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(paddle.nn.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        # 如果是单个整数，则此模块将在最后一个维度上规范化（此时最后一维的维度需与该参数相同）\n",
    "        self.layernorm1 = paddle.nn.LayerNorm(d_model,epsilon=1e-6)\n",
    "        self.layernorm2 = paddle.nn.LayerNorm(d_model,epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = paddle.nn.Dropout(p=rate)\n",
    "        self.dropout2 = paddle.nn.Dropout(p=rate)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        attn_output= self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c22ebca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(paddle.nn.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = paddle.nn.LayerNorm(d_model,epsilon=1e-6)\n",
    "        self.layernorm2 = paddle.nn.LayerNorm(d_model,epsilon=1e-6)\n",
    "        self.layernorm3 = paddle.nn.LayerNorm(d_model,epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = paddle.nn.Dropout(p=rate)\n",
    "        self.dropout2 = paddle.nn.Dropout(p=rate)\n",
    "        self.dropout3 = paddle.nn.Dropout(p=rate)\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "            \n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1= self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2= self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "            \n",
    "        attn2 = self.dropout2(attn2)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d5c6109",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(paddle.nn.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,maximum_position_encoding, rate=0.1):\n",
    "                \n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = paddle.nn.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = get_position_embedding(maximum_position_encoding,self.d_model) \n",
    "                                                \n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "                        \n",
    "\n",
    "        self.dropout =paddle.nn.Dropout(p=rate)\n",
    "\n",
    "    def forward(self, x,  mask):\n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "\n",
    "        # 将嵌入和位置编码相加。\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "     \n",
    "        x *= np.sqrt(self.d_model).astype(np.float32)\n",
    "        \n",
    "       \n",
    "        x +=self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da6b9f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(paddle.nn.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = paddle.nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = get_position_embedding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "                        \n",
    "        self.dropout = paddle.nn.Dropout(p=rate)\n",
    "\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "            \n",
    "        seq_len = x.shape[1]\n",
    "       \n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= np.sqrt(self.d_model).astype(np.float32)\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x= self.dec_layers[i](x, enc_output,look_ahead_mask, padding_mask)\n",
    "                                                \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02933e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(paddle.nn.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = paddle.nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = get_position_embedding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "                        \n",
    "        self.dropout = paddle.nn.Dropout(p=rate)\n",
    "\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "            \n",
    "        seq_len = x.shape[1]\n",
    "       \n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= np.sqrt(self.d_model).astype(np.float32)\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x= self.dec_layers[i](x, enc_output,look_ahead_mask, padding_mask)\n",
    "                                                \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x\n",
    "class Trans(paddle.nn.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,target_vocab_size, pe_input, pe_target, rate=0.1): \n",
    "                \n",
    "        super(Trans, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "                            \n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "                            \n",
    "        self.final_layer = paddle.nn.Linear(d_model,target_vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, inp, tar, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "            \n",
    "        enc_output = self.encoder(inp,  enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output = self.decoder(tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
    "            \n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d0ddd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 256\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0338a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = Trans(num_layers,d_model,num_heads,dff, \n",
    "code_vocab_size,comment_vocab_size,pe_input=code_vocab_size, pe_target=comment_vocab_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2ce7fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------------------------\n",
      " Layer (type)                             Input Shape                                Output Shape         Param #    \n",
      "=======================================================================================================================\n",
      "  Embedding-1                             [[128, 200]]                             [128, 200, 256]       8,171,776   \n",
      "   Dropout-9                           [[128, 200, 256]]                           [128, 200, 256]           0       \n",
      "   Encoder-1                     [[128, 200], [128, 1, 1, 200]]                    [128, 200, 256]           0       \n",
      "  Embedding-2                             [[128, 29]]                               [128, 29, 256]       5,697,536   \n",
      "  Dropout-22                            [[128, 29, 256]]                            [128, 29, 256]           0       \n",
      "   Decoder-1    [[128, 29], [128, 200, 256], [128, 1, 29, 29], [128, 1, 1, 200]]    [128, 29, 256]           0       \n",
      "   Linear-69                            [[128, 29, 256]]                           [128, 29, 22256]      5,719,792   \n",
      "=======================================================================================================================\n",
      "Total params: 19,589,104\n",
      "Trainable params: 19,589,104\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.72\n",
      "Forward/backward pass size (MB): 802.05\n",
      "Params size (MB): 74.73\n",
      "Estimated Total Size (MB): 877.49\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 19589104, 'trainable_params': 19589104}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paddle.summary(trans,[(128,200),(128,29),(128, 1, 1, 200),(128, 1, 29, 29),(128, 1, 1, 200)],dtypes='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "605eb1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "618fba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_loss(a,b):\n",
    "    x_list=[]\n",
    "    for i in range(len(a)):\n",
    "        x_list.append(i)\n",
    "    plt.title(\"LOSS\")\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.plot(x_list,a,marker='s',label=\"train\")\n",
    "    plt.plot(x_list,b,marker='s',label=\"val\")\n",
    "    plt.legend()\n",
    "    plt.savefig('LOSS.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "13355f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    #clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=5.0)\n",
    "    scheduler = paddle.optimizer.lr.NoamDecay(d_model, warmup_steps=4000)# ,verbose=True\n",
    "    #opt=paddle.optimizer.Adam(learning_rate=scheduler,beta1=0.9, beta2=0.98, epsilon=1e-09,parameters=trans.parameters(),grad_clip=clip)\n",
    "    \n",
    "    opt=paddle.optimizer.Adam(learning_rate=scheduler,beta1=0.9, beta2=0.98, epsilon=1e-09,parameters=trans.parameters())\n",
    "\n",
    "    ce_loss = paddle.nn.CrossEntropyLoss(reduction='none')\n",
    "    train_loss=[]\n",
    "    val_loss=[]\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch:{}\".format(epoch))\n",
    "        train_epoch_loss=0\n",
    "\n",
    "        # 此处声明模型训练，使用drpout\n",
    "        trans.train()\n",
    "\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            x_data = data[0]\n",
    "            y_data = data[1]\n",
    "            y_inp=y_data[:,:-1]\n",
    "            y_real=y_data[:,1:]#[batch_size,seq_len]\n",
    "            \n",
    "            \n",
    "            encoder_padding_mask,deocder_mask,encoder_decoder_padding_mask=creat_mask(x_data,y_inp)\n",
    "\n",
    "            pre=trans(x_data,y_inp,encoder_padding_mask,deocder_mask,encoder_decoder_padding_mask)\n",
    "            #[batch_size,seq_len,vocab_size]\n",
    "\n",
    "            batch_loss=ce_loss(pre,y_real)\n",
    "            # 消除padding的0的影响\n",
    "            zeo=paddle.zeros(y_real.shape,y_real.dtype)\n",
    "            mask=paddle.cast(paddle.logical_not(paddle.equal(y_real,zeo)),dtype=pre.dtype)\n",
    "            batch_loss*=mask\n",
    "            batch_loss=paddle.mean(batch_loss)\n",
    "\n",
    "            train_epoch_loss+=batch_loss.numpy()\n",
    "\n",
    "            batch_loss.backward()\n",
    "\n",
    "            if batch_id%100==0:\n",
    "                print('batch=',batch_id,' batch_loss= ',batch_loss.numpy())\n",
    "            # 更新参数\n",
    "            opt.step()\n",
    "            # 梯度清零\n",
    "            opt.clear_grad()\n",
    "            #学习率更新\n",
    "            scheduler.step()\n",
    "            #break\n",
    "        ava_loss=train_epoch_loss/train_batch_num\n",
    "        train_loss.append(ava_loss)\n",
    "        \n",
    "\n",
    "        print(\"train epoch: {}  AVALOSS: {}\\n\".format(epoch,ava_loss))\n",
    "        #break\n",
    "\n",
    "        val_epoch_loss=0\n",
    "        # 声明模型在预测，不使用dropout\n",
    "        trans.eval()\n",
    "        for batch_id, data in enumerate(val_loader()):\n",
    "            x_data = data[0]\n",
    "            y_data = data[1]\n",
    "            y_inp=y_data[:,:-1]\n",
    "            y_real=y_data[:,1:]\n",
    "           \n",
    "            encoder_padding_mask,deocder_mask,encoder_decoder_padding_mask=creat_mask(x_data,y_inp)\n",
    "\n",
    "            pre=trans(x_data,y_inp,encoder_padding_mask,deocder_mask,encoder_decoder_padding_mask)\n",
    "            batch_loss=ce_loss(pre,y_real)\n",
    "            # 消除padding 的0的影响\n",
    "            zeo=paddle.zeros(y_real.shape,y_real.dtype)\n",
    "            mask=paddle.cast(paddle.logical_not(paddle.equal(y_real,zeo)),dtype=pre.dtype)\n",
    "            batch_loss*=mask\n",
    "            batch_loss=paddle.mean(batch_loss)\n",
    "\n",
    "            val_epoch_loss+=batch_loss.numpy()\n",
    "\n",
    "            if batch_id%100==0:\n",
    "                print('batch=',batch_id,' batch_loss= ',batch_loss.numpy())\n",
    "        ava_loss=val_epoch_loss/val_batch_num\n",
    "        \n",
    "        val_loss.append(ava_loss)\n",
    "        print(\"val epoch: {}  AVALOSS: {}\\n\".format(epoch,ava_loss))\n",
    "    # 至此，训练结束。下面绘制loss图\n",
    "    draw_loss(train_loss,val_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611bab54",
   "metadata": {},
   "source": [
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "740a674c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "(InvalidArgument) Tensor holds the wrong type, it holds int, but desires to be int64_t.\n  [Hint: Expected valid == true, but received valid:0 != true:1.] (at C:\\home\\workspace\\Paddle_release\\paddle/fluid/framework/tensor_impl.h:33)\n  [operator < softmax_with_cross_entropy > error]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [73]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [72]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m pre\u001b[38;5;241m=\u001b[39mtrans(x_data,y_inp,encoder_padding_mask,deocder_mask,encoder_decoder_padding_mask)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#[batch_size,seq_len,vocab_size]\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m batch_loss\u001b[38;5;241m=\u001b[39m\u001b[43mce_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpre\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_real\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# 消除padding的0的影响\u001b[39;00m\n\u001b[0;32m     32\u001b[0m zeo\u001b[38;5;241m=\u001b[39mpaddle\u001b[38;5;241m.\u001b[39mzeros(y_real\u001b[38;5;241m.\u001b[39mshape,y_real\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\paddle\\fluid\\dygraph\\layers.py:898\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    893\u001b[0m             parallel_helper\u001b[38;5;241m.\u001b[39m_broadcast_parameters(\n\u001b[0;32m    894\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_built \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 898\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m forward_post_hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_post_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m    901\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m forward_post_hook(\u001b[38;5;28mself\u001b[39m, inputs, outputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\paddle\\nn\\layer\\loss.py:396\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, label)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, label):\n\u001b[1;32m--> 396\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mpaddle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoft_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoft_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_softmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_softmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\paddle\\nn\\functional\\loss.py:1389\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, label, weight, ignore_index, reduction, soft_label, axis, use_softmax, name)\u001b[0m\n\u001b[0;32m   1387\u001b[0m     label \u001b[38;5;241m=\u001b[39m paddle\u001b[38;5;241m.\u001b[39munsqueeze(label, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_dygraph_mode():\n\u001b[1;32m-> 1389\u001b[0m     _, out \u001b[38;5;241m=\u001b[39m \u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax_with_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msoft_label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoft_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mignore_index\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnumeric_stable_mode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maxis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muse_softmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_softmax\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1394\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1395\u001b[0m \n\u001b[0;32m   1396\u001b[0m         \u001b[38;5;66;03m#trans weight from class to sample, shape:N or [N,H,W] for 1d and 2d cases.\u001b[39;00m\n\u001b[0;32m   1397\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m soft_label \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1398\u001b[0m             \u001b[38;5;66;03m# chajchaj:\u001b[39;00m\n\u001b[0;32m   1399\u001b[0m             \u001b[38;5;66;03m# weight's shape is C, where C is class num.\u001b[39;00m\n\u001b[0;32m   1400\u001b[0m             \u001b[38;5;66;03m# for 1d case: label's shape is [N,C], weight_gather's shape is N.\u001b[39;00m\n\u001b[0;32m   1401\u001b[0m             \u001b[38;5;66;03m# for 2d case: label's shape is [N,H,W,C], weight_gather's shape is [N,H,W].\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: (InvalidArgument) Tensor holds the wrong type, it holds int, but desires to be int64_t.\n  [Hint: Expected valid == true, but received valid:0 != true:1.] (at C:\\home\\workspace\\Paddle_release\\paddle/fluid/framework/tensor_impl.h:33)\n  [operator < softmax_with_cross_entropy > error]"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0982e4b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6064273c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
